{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "id": "9INzPcbAUYOK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from PIL import Image,ImageFile\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import copy\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import albumentations as augment\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ef72e103750>"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_val = 420\n",
    "torch.manual_seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "id": "lgtLNNoslyOn"
   },
   "outputs": [],
   "source": [
    "base_data_dir = '../data/artifact'\n",
    "fake_dir = os.path.join(base_data_dir, 'generated')  # Fake artworks directory\n",
    "real_dir = os.path.join(base_data_dir, 'real')  # Real artworks directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eqZfdnU2h0c"
   },
   "source": [
    "ARTWORK DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "id": "JGgx0Qz7Abww"
   },
   "outputs": [],
   "source": [
    "#Class to manage artworks with respect to their authenticity\n",
    "\n",
    "class ArtworkDataset(Dataset):\n",
    "  def __init__(self,links,transform):\n",
    "      self.data = links\n",
    "      self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.data.index.shape[0]\n",
    "    \n",
    "  def __getitem__(self,idx):\n",
    "        img = Image.open(self.data.iloc[idx,0])\n",
    "        label_index = self.data.iloc[idx, 1]\n",
    "        if (img.mode != 'RGB'):\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "          img = self.transform(img)\n",
    "        return img, label_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "id": "YtBEgWxnk6cl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved at ../data/artifact/image_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a CSV file with paths to artwork images and their labels (real or fake)\n",
    "data = [] \n",
    "\n",
    "# Iterate over the fake artworks and add their paths and labels to the list\n",
    "for dirpath, dirnames, filenames in os.walk(fake_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".jpg\"): # only consider jpg files\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            data.append((filepath, \"0\"))\n",
    "\n",
    "\n",
    "# Iterate over the real artworks and add their paths and labels to the list\n",
    "for dirpath, dirnames, filenames in os.walk(real_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            data.append((filepath, \"1\"))  # Label 1 for real artworks\n",
    "\n",
    "# Convert the list \"data\" to a pandas dataframe\n",
    "df = pd.DataFrame(data, columns=[\"path\", \"label\"])\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "csv_output_path = os.path.join(base_data_dir, \"image_labels.csv\")\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "print(f\"CSV file saved at {csv_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5tSRTVLyp14"
   },
   "source": [
    "LOAD PRETRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMq_NPE1pbFG",
    "outputId": "a86e8690-f0ed-40df-f285-137995256b0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (1.0.9)\n",
      "Requirement already satisfied: torch in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from timm) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from timm) (0.19.1)\n",
      "Requirement already satisfied: pyyaml in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from timm) (0.20.3)\n",
      "Requirement already satisfied: safetensors in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from timm) (0.4.4)\n",
      "Requirement already satisfied: filelock in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.6.1)\n",
      "Requirement already satisfied: requests in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from huggingface_hub->timm) (4.11.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from huggingface_hub->timm) (24.1)\n",
      "Requirement already satisfied: sympy in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from torch->timm) (1.13.2)\n",
      "Requirement already satisfied: networkx in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: numpy in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from torchvision->timm) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/oem/miniconda3/envs/python3.10/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ConvNeXt(\n",
      "  (stem): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (1): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (stages): Sequential(\n",
      "    (0): ConvNeXtStage(\n",
      "      (downsample): Identity()\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): ConvNeXtStage(\n",
      "      (downsample): Sequential(\n",
      "        (0): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ConvNeXtStage(\n",
      "      (downsample): Sequential(\n",
      "        (0): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (3): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (4): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (5): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (6): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (7): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (8): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (9): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (10): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (11): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (12): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (13): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (14): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (15): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (16): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (17): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (18): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (19): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (20): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (21): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (22): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (23): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (24): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (25): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (26): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ConvNeXtStage(\n",
      "      (downsample): Sequential(\n",
      "        (0): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm_pre): Identity()\n",
      "  (head): NormMlpClassifierHead(\n",
      "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
      "    (norm): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (pre_logits): Identity()\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "%pip install timm\n",
    "import timm\n",
    "\n",
    "model = timm.create_model('convnext_base',pretrained=True, num_classes=2)\n",
    "\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "id": "VCk48tcMq95u"
   },
   "outputs": [],
   "source": [
    "#Setting the model weights to non-trainable\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "id": "ekf7GcoTrIus"
   },
   "outputs": [],
   "source": [
    "#Make the last layer of the model trainable\n",
    "for p in model.head.parameters(): #instead of fc, we use head\n",
    "    p.requires_grad=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjxaiYnI0er8"
   },
   "source": [
    "SPLIT IN TRAINING AND VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "jAtnYc-86UaP",
    "outputId": "c1d498e5-effb-4815-a275-b3c4df241fc0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/artifact/generated/stylegan3-t-metface...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/artifact/generated/stylegan3-t-metface...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/artifact/generated/stylegan3-t-metface...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/artifact/generated/stylegan3-t-metface...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/artifact/generated/stylegan3-t-metface...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88146</th>\n",
       "      <td>../data/artifact/real/paul-albert-besnard_robe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88147</th>\n",
       "      <td>../data/artifact/real/nikolai-ge_christ-and-th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88148</th>\n",
       "      <td>../data/artifact/real/paul-bril_view-of-a-port...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88149</th>\n",
       "      <td>../data/artifact/real/felix-vallotton_the-port...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88150</th>\n",
       "      <td>../data/artifact/real/nicholas-roerich_tidings...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88151 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  label\n",
       "0      ../data/artifact/generated/stylegan3-t-metface...      0\n",
       "1      ../data/artifact/generated/stylegan3-t-metface...      0\n",
       "2      ../data/artifact/generated/stylegan3-t-metface...      0\n",
       "3      ../data/artifact/generated/stylegan3-t-metface...      0\n",
       "4      ../data/artifact/generated/stylegan3-t-metface...      0\n",
       "...                                                  ...    ...\n",
       "88146  ../data/artifact/real/paul-albert-besnard_robe...      1\n",
       "88147  ../data/artifact/real/nikolai-ge_christ-and-th...      1\n",
       "88148  ../data/artifact/real/paul-bril_view-of-a-port...      1\n",
       "88149  ../data/artifact/real/felix-vallotton_the-port...      1\n",
       "88150  ../data/artifact/real/nicholas-roerich_tidings...      1\n",
       "\n",
       "[88151 rows x 2 columns]"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df\n",
    "dataset['label'] = dataset['label'].astype(int)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {
    "id": "g60Xfpx800Oo"
   },
   "outputs": [],
   "source": [
    "#train, validation = train_test_split(dataset.values, stratify=dataset.values[:, 1], test_size=.3, random_state = 1) \n",
    "\n",
    "# Split the dataset into two parts (train + validation, and test)\n",
    "train_val_data, test = train_test_split(dataset.values, test_size=0.1, random_state=seed_val)\n",
    "\n",
    "# Split the train + validation part into training and validation sets\n",
    "train, validation = train_test_split(train_val_data, test_size=0.1, random_state=seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 7934\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test set size: {len(validation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "id": "uWkbE_RQ1pwq"
   },
   "outputs": [],
   "source": [
    "train_links = pd.DataFrame(train, columns = dataset.columns)\n",
    "validation_links = pd.DataFrame(validation, columns = dataset.columns)\n",
    "test_links = pd.DataFrame(test, columns = dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/artifact/real/felipe-de-vicente_art-67...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/artifact/generated/stylegan3-r-metface...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/artifact/real/johannes-itten_der-bachs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/artifact/generated/stylegan3-t-metface...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/artifact/real/stanley-spencer_kit-insp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path label\n",
       "0  ../data/artifact/real/felipe-de-vicente_art-67...     1\n",
       "1  ../data/artifact/generated/stylegan3-r-metface...     0\n",
       "2  ../data/artifact/real/johannes-itten_der-bachs...     1\n",
       "3  ../data/artifact/generated/stylegan3-t-metface...     0\n",
       "4  ../data/artifact/real/stanley-spencer_kit-insp...     1"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_links[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jHAZeSr3IBk"
   },
   "source": [
    "BUILDING DATA LOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "id": "GMOMejkn65Dp"
   },
   "outputs": [],
   "source": [
    "# old\n",
    "data_transforms = transforms.Compose([\n",
    "                                transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# new\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),                    # Resize to allow cropping\n",
    "    transforms.RandomCrop(224),                # Crop to 224x224\n",
    "    transforms.RandomHorizontalFlip(p=0.5),    # Random horizontal flip -- half of the images are mirrored; prevent the model from learning exact position-based cues.\n",
    "    transforms.RandomRotation(10),             # Small rotation for variability -- helps the model generalize across minor rotations, as real-world artworks and digital images might not always be perfectly aligned.\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1), # Minimal brightness/contrast -- A brightness and contrast jitter of ±10% introduces slight lighting variability without drastically changing color tones. This adjustment accounts for small lighting and contrast differences that can naturally occur in photographed or scanned artworks, which could otherwise become confounding factors.\n",
    "    transforms.ToTensor(),                     # Convert to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Standard normalization\n",
    "])\n",
    "\n",
    "# Validation/test transformations (no augmentation)\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize(224),                    # Resize directly to 224x224\n",
    "    transforms.CenterCrop(224),                # Center crop to ensure consistency\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image\n",
    "])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_set = ArtworkDataset(train_links, train_transforms)\n",
    "\n",
    "validation_set = ArtworkDataset(validation_links, val_test_transforms)\n",
    "\n",
    "test_set = ArtworkDataset(test_links, val_test_transforms)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, \n",
    "                               drop_last=False,num_workers=2)\n",
    "\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False, \n",
    "                               drop_last=False,num_workers=2)\n",
    "\n",
    "test_loader = DataLoader(test_set,batch_size=batch_size, shuffle = False,\n",
    "                              drop_last=False,num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ghMo_wYLTuq"
   },
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "id": "KQv22H68LXcR"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.wait = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = current_loss\n",
    "        elif (current_loss - self.best_loss) < -self.min_delta:\n",
    "            self.best_loss = current_loss\n",
    "            self.wait = 0\n",
    "            \n",
    "             # Save the model state\n",
    "            model_path = os.path.join(self.model_dir, 'RealArt_vs_FakeArt_convnext_base.pt')\n",
    "\n",
    "            # Remove the old model if it exists in the same directory\n",
    "            if os.path.exists(model_path):\n",
    "                os.remove(model_path)\n",
    "            \n",
    "            # save the new model \n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        else:\n",
    "            self.wait = self.wait + 1\n",
    "            print(f\"INFO: Early stopping counter {self.wait} of {self.patience}\")\n",
    "            if self.wait >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "    def __call__(self, current_loss):\n",
    "        if self.best_loss is None or (current_loss - self.best_loss) < -self.min_delta:\n",
    "            self.best_loss = current_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            print(f\"INFO: Early stopping counter {self.wait} of {self.patience}\")\n",
    "            if self.wait >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for logging the misclassified images in tensorboard\n",
    "def denormalize_image(tensor, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)):\n",
    "    \"\"\"Denormalizes a tensor image by applying the inverse of the normalization transform.\"\"\"\n",
    "    # Reshape mean and std to match the (C, H, W) shape of the tensor\n",
    "    mean = torch.tensor(mean).view(3, 1, 1).to(tensor.device)  # Match device as well\n",
    "    std = torch.tensor(std).view(3, 1, 1).to(tensor.device)\n",
    "    denormalized = tensor * std + mean\n",
    "    return denormalized.clamp(0, 1)\n",
    "\n",
    "def add_labels_to_image(image, true_label, pred_label):\n",
    "    \"\"\"Add true and predicted labels to the image.\"\"\"\n",
    "    # Convert the image to uint8 if it's in float format\n",
    "    if image.dtype != np.uint8:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "\n",
    "    # Convert from RGB to BGR for OpenCV\n",
    "    bgr_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Prepare text without brackets\n",
    "    text = f'True: {int(true_label)}, Pred: {int(pred_label)}'\n",
    "    cv2.putText(bgr_image, text, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Convert back to RGB before returning\n",
    "    return cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_misclassified_images(misclassified_images, misclassified_preds, misclassified_true, epoch, writer):\n",
    "    print(\"Logging misclassified images...\")\n",
    "    print(len(misclassified_images))\n",
    "    labeled_images = []\n",
    "\n",
    "    for i, img in enumerate(misclassified_images):\n",
    "        img = denormalize_image(img)  # Denormalize the image\n",
    "        img_np = img.permute(1, 2, 0).numpy()  # Convert (C, H, W) -> (H, W, C)\n",
    "      \n",
    "\n",
    "        # Ensure the image is in the right format for TensorBoard\n",
    "        if img_np.shape[-1] == 1:  # Check if the image is grayscale\n",
    "            img_np = img_np.squeeze(axis=2)  # Remove the channel dimension if it is 1\n",
    "        elif img_np.shape[0] == 1:  # If the shape is (1, H, W)\n",
    "            img_np = img_np.squeeze(axis=0)  # Remove the batch dimension\n",
    "        img_np = np.clip(img_np, 0, 1)  # Ensure the values are between 0 and 1\n",
    "        img_np = (img_np * 255).astype(np.uint8)     # Scale to 0-255\n",
    "\n",
    "        true_label = misclassified_true[i]\n",
    "        pred_label = misclassified_preds[i]\n",
    "\n",
    "        labeled_image = add_labels_to_image(img_np, true_label, pred_label)\n",
    "        # labeled_image = labeled_image.transpose(2, 0, 1)\n",
    "        labeled_images.append(labeled_image)\n",
    "\n",
    "    # Log image to TensorBoard\n",
    "    writer.add_images(\n",
    "        f'Misclassified_Epoch_{epoch}',\n",
    "        np.array(labeled_images),\n",
    "        global_step=epoch,\n",
    "        dataformats='NHWC'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device, epoch, log_name, writer):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Iterate over training data\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch} Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        outputs = nn.Softmax(dim=1)(outputs)  # Apply softmax for classification\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #print(torch.argmax(outputs, dim = 1))\n",
    "        preds = torch.argmax(outputs, dim = 1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # Compute loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / (len(train_loader) * train_loader.batch_size)\n",
    "    epoch_acc = running_corrects.double() / (len(train_loader) * train_loader.batch_size)\n",
    "\n",
    "    # Log statistics\n",
    "    writer.add_scalar(f'Loss/Train/{log_name}', epoch_loss, epoch)\n",
    "    writer.add_scalar(f'Accuracy/Train/{log_name}', epoch_acc, epoch)\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "    return epoch_acc, epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, validation_loader, criterion, device, epoch, num_epochs, log_name, writer):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    misclassified_images = []\n",
    "    misclassified_preds = []\n",
    "    misclassified_true = []\n",
    "\n",
    "    # Disable gradient computation for validation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(validation_loader, desc=f\"Epoch {epoch} Validation\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = nn.Softmax(dim=1)(outputs)  # Apply softmax for classification\n",
    "            #_, preds = torch.max(outputs, 1)\n",
    "            preds = torch.argmax(outputs, dim = 1)\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Collect all predictions and labels for confusion matrix\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "         \n",
    "            # Collect misclassified images\n",
    "            misclassified = preds != labels.data\n",
    "            if misclassified.any():\n",
    "                misclassified_images.extend(inputs[misclassified].cpu())\n",
    "                misclassified_preds.extend(preds[misclassified].cpu().numpy())\n",
    "                misclassified_true.extend(labels[misclassified].cpu().numpy())\n",
    "\n",
    "    # Compute loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / (len(validation_loader) * validation_loader.batch_size)\n",
    "    epoch_acc = running_corrects.double() / (len(validation_loader) * validation_loader.batch_size)\n",
    "\n",
    "    # Log statistics\n",
    "    writer.add_scalar(f'Loss/Validation/{log_name}', epoch_loss, epoch)\n",
    "    writer.add_scalar(f'Accuracy/Validation/{log_name}', epoch_acc, epoch)\n",
    "\n",
    "    print(f\"Validation Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Plot confusion matrix using Seaborn\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Fake', 'True'], yticklabels=['Fake', 'True'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix - Epoch {epoch}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Convert the plot to a NumPy array and log it to TensorBoard\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    image = np.array(Image.open(buf))\n",
    "    writer.add_image(f'Confusion_Matrix', image, global_step=epoch, dataformats='HWC')\n",
    "    buf.close()\n",
    "    plt.close()\n",
    "\n",
    "    # Log misclassified images (with confusion matrix info)\n",
    "    log_misclassified_images(misclassified_images, misclassified_preds, misclassified_true, epoch, writer)\n",
    "\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(model, train_loader, validation_loader, criterion, optimizer, scheduler, early_stop, num_epochs=100, \n",
    "              log_name='run1', device='cuda', writer=None):\n",
    "    best_model = copy.deepcopy(model)\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    stop = False\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        if stop:\n",
    "            break\n",
    "        print(f'Epoch {epoch}/{num_epochs}')\n",
    "        print('-'*120)\n",
    "\n",
    "        # Training phase\n",
    "        train_acc, train_loss = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device, epoch, log_name, writer)\n",
    "\n",
    "        # Validation phase\n",
    "        val_acc, val_loss = validate_one_epoch(model, validation_loader, criterion, device, epoch, num_epochs, log_name, writer)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        early_stop(val_loss)\n",
    "        stop = early_stop.early_stop\n",
    "\n",
    "        print('-'*120)\n",
    "\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    print(f'Best epoch: {best_epoch:03d}')\n",
    "    \n",
    "    # Save the best model state when training stops\n",
    "    model_path = os.path.join('saved_models/convnext_model', 'RealArt_vs_FakeArt_convnext_base.pt')\n",
    "    # torch.save(best_model.state_dict(), model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWDxhbLaMH-G",
    "outputId": "a7d86773-ec64-47d3-d598-0d1de9fafeac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a new model...\n",
      "Epoch 1/30\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/2232 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 2232/2232 [03:11<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3562 Acc: 0.9581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation: 100%|██████████| 248/248 [00:21<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3421 Acc: 0.9703\n",
      "Logging misclassified images...\n",
      "234\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 2/30\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 2232/2232 [03:13<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3445 Acc: 0.9678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation: 100%|██████████| 248/248 [00:21<00:00, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3435 Acc: 0.9688\n",
      "Logging misclassified images...\n",
      "246\n",
      "INFO: Early stopping counter 1 of 3\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 3/30\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 2232/2232 [03:15<00:00, 11.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3419 Acc: 0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation: 100%|██████████| 248/248 [00:21<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3391 Acc: 0.9738\n",
      "Logging misclassified images...\n",
      "206\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 4/30\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 2232/2232 [03:15<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3407 Acc: 0.9712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation: 100%|██████████| 248/248 [00:21<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3385 Acc: 0.9734\n",
      "Logging misclassified images...\n",
      "209\n",
      "INFO: Early stopping counter 1 of 3\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 5/30\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 2232/2232 [03:15<00:00, 11.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3398 Acc: 0.9725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation: 100%|██████████| 248/248 [00:21<00:00, 11.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3398 Acc: 0.9730\n",
      "Logging misclassified images...\n",
      "212\n",
      "INFO: Early stopping counter 2 of 3\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 6/30\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|██████████| 2232/2232 [03:16<00:00, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3391 Acc: 0.9731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation: 100%|██████████| 248/248 [00:22<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3386 Acc: 0.9739\n",
      "Logging misclassified images...\n",
      "205\n",
      "INFO: Early stopping counter 3 of 3\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Best val Acc: 0.973916\n",
      "Best epoch: 006\n",
      "Model saved to: saved_models/convnext_model/RealArt_vs_FakeArt_convnext_base.pt\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join('saved_models/convnext_model', 'RealArt_vs_FakeArt_convnext_base.pt')\n",
    "writer = SummaryWriter(\"runs/confusion_matrix_all\")\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "   print(\"Training a new model...\")\n",
    "   criterion = nn.CrossEntropyLoss()\n",
    "   optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "   scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "   early_stop = EarlyStopping(patience = 3, min_delta = 0.001)\n",
    "   ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "   \n",
    "   # Train and save the best model\n",
    "   best_model_head = fine_tune(model, train_loader, validation_loader, criterion, optimizer, scheduler, \n",
    "                               early_stop, num_epochs = 30, log_name = 'augmented', writer = writer)\n",
    "else: \n",
    "     # Model already exists; load it\n",
    "     print(\"Loading pre-trained model...\")\n",
    "     model = timm.create_model('convnext_base',pretrained=True, num_classes=2) \n",
    "   #   model = torch.load(model_path)  # Load the full model\n",
    "     model.load_state_dict(torch.load(model_path))\n",
    "     print(\"Model loaded successfully from:\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ra1QN4PE6WRX"
   },
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "id": "lVj5Py0WKTkf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# testing function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval() # evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    pred_list = []\n",
    "    true_list = []\n",
    "    misclassified_images = []\n",
    "    misclassified_preds = []\n",
    "    misclassified_true = []\n",
    "\n",
    "\n",
    "    # progress bar\n",
    "    pbar = tqdm(total=len(test_loader))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item() # sommo il loss di ogni batch\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True) # ottengo la predizione del modello\n",
    "            pred_list.extend(pred.cpu().numpy()) # aggiungo la predizione alla lista\n",
    "            true_list.extend(target.cpu().numpy()) # aggiungo il target alla lista\n",
    "            \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # aggiorno il contatore di classificazioni corrette\n",
    "\n",
    "            misclassified = ~pred.eq(target.view_as(pred)).squeeze()\n",
    "            if misclassified.any():\n",
    "                misclassified_images.extend(data[misclassified].cpu())\n",
    "                misclassified_preds.extend(pred[misclassified].cpu().numpy())\n",
    "                misclassified_true.extend(target[misclassified].cpu().numpy())\n",
    "\n",
    "            # update progress bar\n",
    "            pbar.update(1)\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    recall = recall_score(true_list, pred_list, average='macro')\n",
    "    precision = precision_score(true_list, pred_list, average='macro') \n",
    "    f1 = f1_score(true_list, pred_list, average='macro') \n",
    "    auc = roc_auc_score(true_list, pred_list) \n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%), Recall: {:.2f}%, Precision: {:.2f}%, F1: {:.2f}%, AUC: {:.2f}%\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), accuracy, recall*100, precision*100, f1*100, auc*100))\n",
    "    \n",
    "    #Logging the misclassified images\n",
    "    for i, img in enumerate(misclassified_images):\n",
    "        img_denorm = denormalize_image(img.clone())\n",
    "        img_np = img_denorm.permute(1, 2, 0).numpy()  # Convert tensor to numpy (C, H, W) to (H, W, C)\n",
    "        \n",
    "        # Ensure the image is in the right format for TensorBoard\n",
    "        if img_np.shape[-1] == 1:  # Check if the image is grayscale\n",
    "            img_np = img_np.squeeze(axis=2)  # Remove the channel dimension if it is 1\n",
    "        elif img_np.shape[0] == 1:  # If the shape is (1, H, W)\n",
    "            img_np = img_np.squeeze(axis=0)  # Remove the batch dimension\n",
    "\n",
    "        img_np = np.clip(img_np, 0, 1)  # Ensure the values are between 0 and 1\n",
    "        img_np = (img_np * 255).astype(np.uint8)  # Scale to 0-255 and convert to uint8\n",
    "\n",
    "        labeled_image = add_labels_to_image(img_np, misclassified_true[i], misclassified_preds[i])\n",
    "\n",
    "        # Log the image to TensorBoard\n",
    "        writer.add_image(f'Misclassified_True_{misclassified_true[i]}_Pred_{misclassified_preds[i]}_Index_{i}', labeled_image, dataformats='HWC')\n",
    "\n",
    "    writer.flush()\n",
    "    return accuracy, recall, precision, f1, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use this for logs:\n",
    "tensorboard --logdir=./runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FW2GesMHKf11",
    "outputId": "30e6c98e-eab6-4fe8-a641-9c61b3ac059e"
   },
   "outputs": [],
   "source": [
    "# accuracy,recall,precision,f1,auc = test_model(model,test_loader)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
