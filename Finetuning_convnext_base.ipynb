{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "9INzPcbAUYOK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from PIL import Image,ImageFile\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import copy\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam import GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from captum.attr import IntegratedGradients\n",
    "import timm\n",
    "from torchvision.utils import make_grid\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "from torchcam.utils import overlay_mask\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x75efc822ee30>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_val = 420\n",
    "torch.manual_seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "lgtLNNoslyOn"
   },
   "outputs": [],
   "source": [
    "base_data_dir = '../data/artifact_lowFrequencies'\n",
    "fake_dir = os.path.join(base_data_dir, 'generated')  # Fake artworks directory\n",
    "real_dir = os.path.join(base_data_dir, 'real')  # Real artworks directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eqZfdnU2h0c"
   },
   "source": [
    "ARTWORK DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "JGgx0Qz7Abww"
   },
   "outputs": [],
   "source": [
    "#Class to manage artworks with respect to their authenticity\n",
    "\n",
    "class ArtworkDataset(Dataset):\n",
    "  def __init__(self,links,transform):\n",
    "      self.data = links\n",
    "      self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.data.index.shape[0]\n",
    "    \n",
    "  def __getitem__(self,idx):\n",
    "        img = Image.open(self.data.iloc[idx,0])\n",
    "        label_index = self.data.iloc[idx, 1]\n",
    "        if (img.mode != 'RGB'):\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "          img = self.transform(img)\n",
    "        return img, label_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "YtBEgWxnk6cl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved at ../data/artifact_lowFrequencies/image_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a CSV file with paths to artwork images and their labels (real or fake)\n",
    "data = [] \n",
    "\n",
    "# Iterate over the fake artworks and add their paths and labels to the list\n",
    "for dirpath, dirnames, filenames in os.walk(fake_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".jpg\"): # only consider jpg files\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            data.append((filepath, \"0\"))\n",
    "\n",
    "\n",
    "# Iterate over the real artworks and add their paths and labels to the list\n",
    "for dirpath, dirnames, filenames in os.walk(real_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            data.append((filepath, \"1\"))  # Label 1 for real artworks\n",
    "\n",
    "# Convert the list \"data\" to a pandas dataframe\n",
    "df = pd.DataFrame(data, columns=[\"path\", \"label\"])\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "csv_output_path = os.path.join(base_data_dir, \"image_labels.csv\")\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "print(f\"CSV file saved at {csv_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5tSRTVLyp14"
   },
   "source": [
    "LOAD PRETRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMq_NPE1pbFG",
    "outputId": "a86e8690-f0ed-40df-f285-137995256b0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXt(\n",
      "  (stem): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (1): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (stages): Sequential(\n",
      "    (0): ConvNeXtStage(\n",
      "      (downsample): Identity()\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): ConvNeXtStage(\n",
      "      (downsample): Sequential(\n",
      "        (0): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ConvNeXtStage(\n",
      "      (downsample): Sequential(\n",
      "        (0): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (3): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (4): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (5): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (6): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (7): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (8): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (9): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (10): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (11): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (12): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (13): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (14): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (15): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (16): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (17): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (18): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (19): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (20): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (21): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (22): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (23): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (24): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (25): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (26): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ConvNeXtStage(\n",
      "      (downsample): Sequential(\n",
      "        (0): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm_pre): Identity()\n",
      "  (head): Sequential(\n",
      "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n",
      "    (norm): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('convnext_base',pretrained=True, num_classes=2)\n",
    "\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "VCk48tcMq95u"
   },
   "outputs": [],
   "source": [
    "#Setting the model weights to non-trainable\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "ekf7GcoTrIus"
   },
   "outputs": [],
   "source": [
    "#Make the last layer of the model trainable\n",
    "for p in model.head.parameters(): #instead of fc, we use head\n",
    "    p.requires_grad=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjxaiYnI0er8"
   },
   "source": [
    "SPLIT IN TRAINING AND VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "jAtnYc-86UaP",
    "outputId": "c1d498e5-effb-4815-a275-b3c4df241fc0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/artifact_lowFrequencies/generated/styl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/artifact_lowFrequencies/generated/styl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/artifact_lowFrequencies/generated/styl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/artifact_lowFrequencies/generated/styl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/artifact_lowFrequencies/generated/styl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88146</th>\n",
       "      <td>../data/artifact_lowFrequencies/real/paul-albe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88147</th>\n",
       "      <td>../data/artifact_lowFrequencies/real/nikolai-g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88148</th>\n",
       "      <td>../data/artifact_lowFrequencies/real/paul-bril...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88149</th>\n",
       "      <td>../data/artifact_lowFrequencies/real/felix-val...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88150</th>\n",
       "      <td>../data/artifact_lowFrequencies/real/nicholas-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88151 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  label\n",
       "0      ../data/artifact_lowFrequencies/generated/styl...      0\n",
       "1      ../data/artifact_lowFrequencies/generated/styl...      0\n",
       "2      ../data/artifact_lowFrequencies/generated/styl...      0\n",
       "3      ../data/artifact_lowFrequencies/generated/styl...      0\n",
       "4      ../data/artifact_lowFrequencies/generated/styl...      0\n",
       "...                                                  ...    ...\n",
       "88146  ../data/artifact_lowFrequencies/real/paul-albe...      1\n",
       "88147  ../data/artifact_lowFrequencies/real/nikolai-g...      1\n",
       "88148  ../data/artifact_lowFrequencies/real/paul-bril...      1\n",
       "88149  ../data/artifact_lowFrequencies/real/felix-val...      1\n",
       "88150  ../data/artifact_lowFrequencies/real/nicholas-...      1\n",
       "\n",
       "[88151 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df\n",
    "dataset['label'] = dataset['label'].astype(int)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "g60Xfpx800Oo"
   },
   "outputs": [],
   "source": [
    "#train, validation = train_test_split(dataset.values, stratify=dataset.values[:, 1], test_size=.3, random_state = 1) \n",
    "\n",
    "# Split the dataset into two parts (train + validation, and test)\n",
    "train_val_data, test = train_test_split(dataset.values, test_size=0.1, random_state=seed_val)\n",
    "\n",
    "# Split the train + validation part into training and validation sets\n",
    "train, validation = train_test_split(train_val_data, test_size=0.1, random_state=seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 8816\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test set size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "uWkbE_RQ1pwq"
   },
   "outputs": [],
   "source": [
    "train_links = pd.DataFrame(train, columns = dataset.columns)\n",
    "validation_links = pd.DataFrame(validation, columns = dataset.columns)\n",
    "test_links = pd.DataFrame(test, columns = dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/artifact_lowFrequencies/real/felipe-de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/artifact_lowFrequencies/generated/styl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/artifact_lowFrequencies/real/johannes-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/artifact_lowFrequencies/generated/styl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/artifact_lowFrequencies/real/stanley-s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path label\n",
       "0  ../data/artifact_lowFrequencies/real/felipe-de...     1\n",
       "1  ../data/artifact_lowFrequencies/generated/styl...     0\n",
       "2  ../data/artifact_lowFrequencies/real/johannes-...     1\n",
       "3  ../data/artifact_lowFrequencies/generated/styl...     0\n",
       "4  ../data/artifact_lowFrequencies/real/stanley-s...     1"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_links[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jHAZeSr3IBk"
   },
   "source": [
    "#### DATA LOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "GMOMejkn65Dp"
   },
   "outputs": [],
   "source": [
    "# old\n",
    "data_transforms = transforms.Compose([\n",
    "                                transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# new\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),                    # Resize to allow cropping\n",
    "    transforms.RandomCrop(224),                # Crop to 224x224\n",
    "    transforms.RandomHorizontalFlip(p=0.5),    # Random horizontal flip -- half of the images are mirrored; prevent the model from learning exact position-based cues.\n",
    "    transforms.RandomRotation(10),             # Small rotation for variability -- helps the model generalize across minor rotations, as real-world artworks and digital images might not always be perfectly aligned.\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1), # Minimal brightness/contrast -- A brightness and contrast jitter of ±10% introduces slight lighting variability without drastically changing color tones. This adjustment accounts for small lighting and contrast differences that can naturally occur in photographed or scanned artworks, which could otherwise become confounding factors.\n",
    "    transforms.ToTensor(),                     # Convert to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Standard normalization\n",
    "])\n",
    "\n",
    "# Validation/test transformations (no augmentation)\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize(224),                    # Resize directly to 224x224\n",
    "    transforms.CenterCrop(224),                # Center crop to ensure consistency\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image\n",
    "])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_set = ArtworkDataset(train_links, train_transforms)\n",
    "\n",
    "validation_set = ArtworkDataset(validation_links, val_test_transforms)\n",
    "\n",
    "test_set = ArtworkDataset(test_links, val_test_transforms)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, \n",
    "                               drop_last=False,num_workers=6, pin_memory = True)\n",
    "\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False, \n",
    "                               drop_last=False,num_workers=6, pin_memory = True)\n",
    "\n",
    "test_loader = DataLoader(test_set,batch_size=batch_size, shuffle = False,\n",
    "                              drop_last=False,num_workers=6, pin_memory = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ghMo_wYLTuq"
   },
   "source": [
    "**Early Stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "KQv22H68LXcR"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.wait = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "                \n",
    "    def __call__(self, current_loss):\n",
    "        if self.best_loss is None or (current_loss - self.best_loss) < -self.min_delta:\n",
    "            self.best_loss = current_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            print(f\"INFO: Early stopping counter {self.wait} of {self.patience}\")\n",
    "            if self.wait >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions for visualisation and logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for logging the misclassified images in tensorboard\n",
    "def denormalize_image(tensor, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)):\n",
    "    \"\"\"Denormalizes a tensor image by applying the inverse of the normalization transform.\"\"\"\n",
    "    # Reshape mean and std to match the (C, H, W) shape of the tensor\n",
    "    if tensor.dim() == 4:\n",
    "        tensor = tensor[0]\n",
    "    mean = torch.tensor(mean).view(3, 1, 1).to(tensor.device)  # Match device as well\n",
    "    std = torch.tensor(std).view(3, 1, 1).to(tensor.device)\n",
    "    denormalized = tensor * std + mean\n",
    "    denormalized = denormalized.clamp(0, 1) # convert to [H, W, C] for matplotlib visualisation\n",
    "\n",
    "    return denormalized.permute(1, 2, 0).cpu().detach().numpy()\n",
    "\n",
    "def add_labels_to_image(image, true_label, pred_label):\n",
    "    \"\"\"Add true and predicted labels to the image.\"\"\"\n",
    "    # Convert the image to uint8 if it's in float format\n",
    "    if image.dtype != np.uint8:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "\n",
    "    # Convert from RGB to BGR for OpenCV\n",
    "    bgr_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Prepare text without brackets\n",
    "    text = f'True: {int(true_label)}, Pred: {int(pred_label)}'\n",
    "    cv2.putText(bgr_image, text, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Convert back to RGB before returning\n",
    "    return cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_misclassified_images(misclassified_images, misclassified_preds, misclassified_true, epoch, writer):\n",
    "    print(\"Logging misclassified images...\")\n",
    "    print(len(misclassified_images))\n",
    "    labeled_images = []\n",
    "\n",
    "    for i, img in enumerate(misclassified_images):\n",
    "        img_np = denormalize_image(img)  # Denormalize the image      \n",
    "\n",
    "        # Ensure the image is in the right format for TensorBoard\n",
    "        if img_np.shape[-1] == 1:  # Check if the image is grayscale\n",
    "            img_np = img_np.squeeze(axis=2)  # Remove the channel dimension if it is 1\n",
    "        elif img_np.shape[0] == 1:  # If the shape is (1, H, W)\n",
    "            img_np = img_np.squeeze(axis=0)  # Remove the batch dimension\n",
    "        img_np = np.clip(img_np, 0, 1)  # Ensure the values are between 0 and 1\n",
    "        img_np = (img_np * 255).astype(np.uint8)     # Scale to 0-255\n",
    "\n",
    "        true_label = misclassified_true[i]\n",
    "        pred_label = misclassified_preds[i]\n",
    "\n",
    "        labeled_image = add_labels_to_image(img_np, true_label, pred_label)\n",
    "        # labeled_image = labeled_image.transpose(2, 0, 1)\n",
    "        labeled_images.append(labeled_image)\n",
    "\n",
    "    # Log image to TensorBoard\n",
    "    writer.add_images(\n",
    "        f'Misclassified_Epoch_{epoch}',\n",
    "        np.array(labeled_images),\n",
    "        global_step=epoch,\n",
    "        dataformats='NHWC'\n",
    "    )\n",
    "\n",
    "\n",
    "def log_confusion_matrix(all_labels, all_preds, epoch, log_name, writer):\n",
    "    \"\"\"Log confusion matrix to TensorBoard.\"\"\"\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Plot confusion matrix using Seaborn\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Fake', 'True'], yticklabels=['Fake', 'True'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix - {log_name}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save to buffer and log\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    image = np.array(Image.open(buf))\n",
    "    writer.add_image(f'Confusion_Matrix/{log_name}', image, global_step=epoch, dataformats='HWC')\n",
    "    buf.close()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GRADCAM and Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXtBlock(\n",
      "  (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.stages[-1].blocks[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_integrated_gradients_old(model, inputs, labels, device, epoch, writer):\n",
    "    \"\"\"\n",
    "    Applies Integrated Gradients to a single batch of inputs and logs a side-by-side plot (input + IG) to TensorBoard.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ig = IntegratedGradients(model)\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Perform IG for a single input (e.g., the first image in the batch)\n",
    "    input_example = inputs[0].unsqueeze(0)  # Add batch dimension\n",
    "    target_label = labels[0].item()  # Ground truth class\n",
    "\n",
    "    # Compute attributions\n",
    "    attributions = ig.attribute(input_example, target=target_label, n_steps=50)\n",
    "\n",
    "    # Convert tensors to numpy for visualization\n",
    "    input_example = input_example.squeeze().cpu().detach().numpy()\n",
    "    attributions = attributions.squeeze().cpu().detach().numpy()\n",
    "\n",
    "    # Normalize attributions for visualization\n",
    "    attributions = (attributions - attributions.min()) / (attributions.max() - attributions.min())\n",
    "\n",
    "    # Reshape input and attribution for RGB or Grayscale\n",
    "    input_example = input_example.transpose(1, 2, 0)  # CHW to HWC for RGB\n",
    "    if input_example.shape[2] != 3:  # Handle grayscale\n",
    "        input_example = np.repeat(input_example[:, :, np.newaxis], 3, axis=2)\n",
    "    attribution_overlay = attributions.transpose(1, 2, 0)\n",
    "\n",
    "    # Create a side-by-side plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(input_example)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].imshow(attribution_overlay, cmap=\"viridis\")\n",
    "    axes[1].set_title(\"Integrated Gradients\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # Save the plot to a buffer\n",
    "    fig.canvas.draw()\n",
    "    buf = fig.canvas.tostring_rgb()\n",
    "    width, height = fig.canvas.get_width_height()\n",
    "    image = np.frombuffer(buf, dtype=np.uint8).reshape(height, width, 3).transpose(2, 0, 1)  # HWC to CHW\n",
    "\n",
    "    plt.close(fig)  # Close the plot to free memory\n",
    "\n",
    "    # Log the combined image to TensorBoard\n",
    "    writer.add_image(f\"IG/Input_and_IG\", image, epoch)\n",
    "\n",
    "    print(f\"Integrated Gradients side-by-side plot logged for epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gradcam(model, image_tensor, true_label, class_idx, writer, log_name=\"Validation\"):\n",
    "    \"\"\"\n",
    "    Apply Grad-CAM++ to visualize class-specific saliency maps and overlay them on the input image.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model.\n",
    "        target_layer: The name of the layer to use for Grad-CAM++.\n",
    "        image_tensor: The input image tensor (normalized, shape CxHxW).\n",
    "        true_label: The true label of the image (int).\n",
    "        class_idx: The predicted class index to visualize (int).\n",
    "        writer: TensorBoard writer for logging.\n",
    "        step: The current step (e.g., batch index or global step).\n",
    "        denormalize: A function to denormalize the image tensor for visualization.\n",
    "    \"\"\"\n",
    "    # Move model to eval mode\n",
    "    model.eval()\n",
    "    image_tensor.requires_grad = True\n",
    "    target_layer = model.stages[-1].blocks[-1].conv_dw\n",
    "\n",
    "    # Create Grad-CAM++ object\n",
    "    cam_extractor = SmoothGradCAMpp(model, target_layer)\n",
    "\n",
    "    # Detach the image for visualization\n",
    "    img = image_tensor.detach()\n",
    "\n",
    "    model(image_tensor)\n",
    "    \n",
    "    # Denormalize for visual output\n",
    "    img_np = denormalize_image(img)\n",
    "    img_np = (img_np * 255).astype('uint8')\n",
    "    pil_image = Image.fromarray(img_np)\n",
    "\n",
    "    if not isinstance(class_idx, (list, np.ndarray)):\n",
    "        class_idx = [class_idx]\n",
    "\n",
    "    # Generate Grad-CAM++ activation maps\n",
    "    cams = cam_extractor(class_idx, img.unsqueeze(0))\n",
    "    cam_map = cams[0].squeeze(0).cpu()\n",
    "\n",
    "    # Overlay Grad-CAM++ heatmap on the original image\n",
    "    heatmap = overlay_mask(pil_image, Image.fromarray((cam_map.numpy() * 255).astype('uint8')), alpha=0.5)\n",
    "\n",
    "    # Plot original image and overlayed heatmap\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].imshow(pil_image)\n",
    "    ax[0].set_title(f\"Original Image (Label: {true_label})\")\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[1].imshow(heatmap)\n",
    "    ax[1].set_title(f\"Grad-CAM++ (Class: {class_idx})\")\n",
    "    ax[1].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Log images to TensorBoard\n",
    "    writer.add_figure(f\"GradCAM++/{log_name}\", fig)\n",
    "\n",
    "    plt.close(fig)  # Close the plot to free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_integrated_gradients(\n",
    "    model, input_tensor, target_label, baseline=None, writer=None, log_name = \"Validation\"):\n",
    "    \"\"\"\n",
    "    Visualizes Integrated Gradients (IG) for a single input image.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model.\n",
    "        input_tensor: Input image tensor (C, H, W) normalized for the model.\n",
    "        target_label: Target label for IG attribution.\n",
    "        baseline: Baseline for IG. Defaults to zero baseline.\n",
    "        writer: TensorBoard SummaryWriter object.\n",
    "        log_name: Name for logging the figure (default: \"Validation\").\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the target_label is a PyTorch tensor of type long\n",
    "    target_label = torch.tensor(target_label, dtype=torch.long).to(input_tensor.device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Ensure that the input tensor requires gradients\n",
    "    if input_tensor.dim() == 3:  # Shape: [C, H, W]\n",
    "        input_tensor = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "    input_tensor.requires_grad_()  # Enable gradients for backpropagation\n",
    "\n",
    "\n",
    "    input_tensor.requires_grad_()  # Ensure the input tensor requires gradients\n",
    "\n",
    "    # Prepare baseline (default is zero image)\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(input_tensor).to(input_tensor.device)\n",
    "    baseline.requires_grad = False\n",
    "\n",
    "    \n",
    "    # Compute attributions using Integrated Gradients\n",
    "    ig = IntegratedGradients(model)\n",
    "    attributions, _ = ig.attribute(\n",
    "        inputs=input_tensor, \n",
    "        baselines=baseline, \n",
    "        target=target_label, \n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "\n",
    "    print(f\"Attributions shape before processing: {attributions.shape}\")\n",
    "\n",
    "    \n",
    "    # Process the attributions\n",
    "    attributions = attributions.squeeze(0).cpu().detach()  # Shape: (C, H, W)\n",
    "    print(f\"Attributions shape after squeeze: {attributions.shape}\")\n",
    "\n",
    "    \n",
    "    # Normalize IG attributions to [0, 1] for visualization\n",
    "    attributions_normalized = (attributions - attributions.min()) / (attributions.max() - attributions.min())\n",
    "    attributions_normalized = attributions_normalized.permute(1, 2, 0).numpy()  # Shape: (H, W, C)\n",
    "    \n",
    "    # Denormalize the input image for visualization\n",
    "    input_image_denormalized = denormalize_image(input_tensor.squeeze(0)).permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # Plotting the results\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].imshow(input_image_denormalized)\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('Denormalized Input Image')\n",
    "\n",
    "    ax[1].imshow(input_image_denormalized, alpha=0.6)  # Overlay original image\n",
    "    ax[1].imshow(attributions_normalized, cmap='hot', alpha=0.4)  # Overlay IG heatmap\n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title('Integrated Gradients Attributions')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure to TensorBoard if writer is provided\n",
    "    plt.show()\n",
    "    writer.add_figure(f\"Integrated Gradients/{log_name}\", fig)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING AND VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device, epoch, log_name, writer):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Iterate over training data\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch} Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        outputs = nn.Softmax(dim=1)(outputs)  # Apply softmax for classification\n",
    "        preds = torch.argmax(outputs, dim = 1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # Compute loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / (len(train_loader) * train_loader.batch_size)\n",
    "    epoch_acc = running_corrects.double() / (len(train_loader) * train_loader.batch_size)\n",
    "\n",
    "    # Log statistics\n",
    "    writer.add_scalar(f'Loss/Train/{log_name}', epoch_loss, epoch)\n",
    "    writer.add_scalar(f'Accuracy/Train/{log_name}', epoch_acc, epoch)\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "    return epoch_acc, epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_heatmap_on_image(image, heatmap, alpha=0.6):\n",
    "    \"\"\"\n",
    "    Overlay a heatmap on an image.\n",
    "    \n",
    "    Args:\n",
    "        image: The original image tensor (C, H, W).\n",
    "        heatmap: The heatmap (H, W) to overlay.\n",
    "        alpha: Transparency factor for the overlay (0 to 1).\n",
    "        \n",
    "    Returns:\n",
    "        The image with the overlaid heatmap.\n",
    "    \"\"\"\n",
    "    # Convert tensor image from (C, H, W) to (H, W, C) and denormalize it\n",
    "\n",
    "    # Resize the heatmap to match the image size\n",
    "    heatmap_resized = np.resize(heatmap, (image.shape[0], image.shape[1]))\n",
    "\n",
    "    # Normalize heatmap to [0, 1]\n",
    "    heatmap_resized = np.maximum(heatmap_resized, 0)  # Avoid negative values\n",
    "    heatmap_resized = heatmap_resized / np.max(heatmap_resized) \n",
    "\n",
    "    # Apply the heatmap on the image (using alpha blending)\n",
    "    heatmap_colored = plt.cm.jet(heatmap_resized)[:, :, :3]  # Convert heatmap to RGB\n",
    "    overlay = image * (1 - alpha) + heatmap_colored * alpha\n",
    "    overlay = (overlay * 255).astype(np.uint8)\n",
    "    return overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, validation_loader, criterion, device, epoch, num_epochs, log_name, writer):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    misclassified_images = []\n",
    "    misclassified_preds = []\n",
    "    misclassified_true = []\n",
    "\n",
    "\n",
    "    def save_activation(module, input, output):\n",
    "        activations.append(output)\n",
    "\n",
    "    def save_gradients(module, input, output):\n",
    "        grads.append(output[0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(validation_loader, desc=f\"Epoch {epoch} Validation\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = nn.Softmax(dim=1)(outputs)  # Apply softmax for classification\n",
    "            preds = torch.argmax(outputs, dim = 1)\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Collect all predictions and labels for confusion matrix\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Collect misclassified images\n",
    "            misclassified = preds != labels.data\n",
    "            if misclassified.any():\n",
    "                misclassified_images.extend(inputs[misclassified].cpu())\n",
    "                misclassified_preds.extend(preds[misclassified].cpu().numpy())\n",
    "                misclassified_true.extend(labels[misclassified].cpu().numpy())\n",
    "\n",
    "\n",
    "    # Compute loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / (len(validation_loader) * validation_loader.batch_size)\n",
    "    epoch_acc = running_corrects.double() / (len(validation_loader) * validation_loader.batch_size)\n",
    "\n",
    "    # Log statistics\n",
    "    writer.add_scalar(f'Loss/Validation/{log_name}', epoch_loss, epoch)\n",
    "    writer.add_scalar(f'Accuracy/Validation/{log_name}', epoch_acc, epoch)\n",
    "\n",
    "    print(f\"Validation Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    log_confusion_matrix(all_labels, all_preds, epoch, log_name, writer)\n",
    "    \n",
    "    # Log misclassified images (with confusion matrix info)\n",
    "    # log_misclassified_images(misclassified_images, misclassified_preds, misclassified_true, epoch, writer)\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return epoch_acc, epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(model, train_loader, validation_loader, criterion, optimizer, scheduler, early_stop, num_epochs=100, \n",
    "              log_name='run1', device='cuda', writer=None):\n",
    "    best_model = copy.deepcopy(model)\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    stop = False\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        if stop:\n",
    "            break\n",
    "        print(f'Epoch {epoch}/{num_epochs}')\n",
    "        print('-'*120)\n",
    "\n",
    "        # Training phase\n",
    "        train_acc, train_loss = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device, epoch, log_name, writer)\n",
    "\n",
    "        # Validation phase\n",
    "        val_acc, val_loss = validate_one_epoch(model, validation_loader, criterion, device, epoch, num_epochs, log_name, writer)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        early_stop(val_loss)\n",
    "        stop = early_stop.early_stop\n",
    "\n",
    "        print('-'*120)\n",
    "\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    print(f'Best epoch: {best_epoch:03d}')\n",
    "    \n",
    "    # Save the best model state when training stops\n",
    "    model_path = os.path.join('saved_models/convnext_model', 'RealArt_vs_FakeArt_convnext_base.pt')\n",
    "    # torch.save(best_model.state_dict(), model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWDxhbLaMH-G",
    "outputId": "a7d86773-ec64-47d3-d598-0d1de9fafeac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a new model...\n",
      "Epoch 1/30\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/2232 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 2232/2232 [03:02<00:00, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3952 Acc: 0.9213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation: 100%|██████████| 248/248 [00:20<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3803 Acc: 0.9327\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 2/30\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 2232/2232 [03:05<00:00, 12.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3765 Acc: 0.9342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation: 100%|██████████| 248/248 [00:21<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3811 Acc: 0.9306\n",
      "INFO: Early stopping counter 1 of 3\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 3/30\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 2232/2232 [03:06<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3726 Acc: 0.9391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation: 100%|██████████| 248/248 [00:21<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3887 Acc: 0.9240\n",
      "INFO: Early stopping counter 2 of 3\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 4/30\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 2232/2232 [03:07<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3713 Acc: 0.9396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation: 100%|██████████| 248/248 [00:21<00:00, 11.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4009 Acc: 0.9109\n",
      "INFO: Early stopping counter 3 of 3\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Best val Acc: 0.932712\n",
      "Best epoch: 001\n",
      "Model saved to: saved_models/convnext_model/RealArt_vs_FakeArt_convnext_base.pt\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join('saved_models/convnext_model', 'RealArt_vs_FakeArt_convnext_base1.pt')\n",
    "writer = SummaryWriter(\"runs/train_low_freq/take2\")\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "   print(\"Training a new model...\")\n",
    "   criterion = nn.CrossEntropyLoss()\n",
    "   optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "   scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "   early_stop = EarlyStopping(patience = 3, min_delta = 0.001)\n",
    "   ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "   \n",
    "   # Train and save the best model\n",
    "   best_model_head = fine_tune(model, train_loader, validation_loader, criterion, optimizer, scheduler, \n",
    "                               early_stop, num_epochs = 30, writer = writer)\n",
    "else: \n",
    "     # Model already exists; load it\n",
    "     print(\"Loading pre-trained model...\")\n",
    "     model = timm.create_model('convnext_base',pretrained=True, num_classes=2) \n",
    "     model.load_state_dict(torch.load(model_path))\n",
    "     model.to(device)\n",
    "     print(\"Model loaded successfully from:\", model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ra1QN4PE6WRX"
   },
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXtBlock(\n",
      "  (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.stages[-1].blocks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_image_gradcam(tensor, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)):\n",
    "    \"\"\"Denormalizes a tensor image by applying the inverse of the normalization transform.\"\"\"\n",
    "    # Reshape mean and std to match the (C, H, W) shape of the tensor\n",
    "    mean = torch.tensor(mean).view(3, 1, 1).to(tensor.device)  # Match device as well\n",
    "    std = torch.tensor(std).view(3, 1, 1).to(tensor.device)\n",
    "    denormalized = tensor * std + mean\n",
    "    return denormalized.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_grad_cam_on_image(original_image, cam_image, alpha=0.5, scale_factor=2):\n",
    "    \"\"\"Overlays the Grad-CAM image on the original image with some transparency.\"\"\"\n",
    "    # Ensure the original image is in [0, 1] range if it's in [0, 255]\n",
    "    if original_image.max() > 1:  # This means the image is in [0, 255] range\n",
    "        original_image = original_image.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "    # Normalize Grad-CAM to [0, 1]\n",
    "    cam_image = cam_image.astype(np.float32)  # Ensure float32 type\n",
    "    cam_image = np.clip(cam_image, 0, 1)  # Clip to make sure it's within [0, 1]\n",
    "\n",
    "    # Apply a scaling factor to the Grad-CAM image\n",
    "    cam_image = cam_image * scale_factor\n",
    "    cam_image = np.clip(cam_image, 0, 1)  # Make sure the scaled cam_image stays within [0, 1]\n",
    "\n",
    "    # Apply a colormap to the Grad-CAM image to get the heatmap effect (rainbow)\n",
    "    cam_image = plt.get_cmap('jet')(cam_image)  # Apply the 'jet' colormap\n",
    "\n",
    "    # The colormap adds an extra dimension, we need to remove it\n",
    "    cam_image = cam_image[..., :3]  # Take the RGB channels (ignore the alpha channel)\n",
    "\n",
    "    # Apply overlay with transparency (alpha blending)\n",
    "    overlay = original_image * (1 - alpha) + cam_image * alpha\n",
    "    overlay = np.clip(overlay, 0, 1)  # Ensure values are valid for display\n",
    "\n",
    "    # Convert to uint8 for visualization\n",
    "    overlay_uint8 = (overlay * 255).astype(np.uint8)\n",
    "    \n",
    "    return overlay_uint8\n",
    "\n",
    "def apply_grad_cam(model, images, preds, true_labels, writer):\n",
    "    \"\"\"Applies Grad-CAM++ to misclassified images and logs them to TensorBoard.\"\"\"\n",
    "    target_layers = [model.stages[-1].blocks[-1].conv_dw]  # Example target layer\n",
    "    print(\"Using target layer:\", target_layers)\n",
    "\n",
    "    cam = GradCAMPlusPlus(model=model, target_layers=target_layers)\n",
    "\n",
    "    for idx, (image, true, pred) in enumerate(zip(images, true_labels, preds)):\n",
    "        # Denormalize the image to [0, 1] range\n",
    "        image = denormalize_image_gradcam(image, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        image_np = image.numpy().transpose(1, 2, 0)  # Convert tensor to numpy (H, W, C)\n",
    "\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "         # Apply softmax to the logits to get probabilities\n",
    "        image = image.to(device)\n",
    "        \n",
    "        # Generate Grad-CAM visualization\n",
    "        grayscale_cam = cam(input_tensor=image)\n",
    "\n",
    "        # Overlay Grad-CAM on the original image with transparency\n",
    "        overlay_image = overlay_grad_cam_on_image(image_np, grayscale_cam[0], alpha=0.5)\n",
    "\n",
    "        # Add labels to the image\n",
    "        overlay_image_with_labels = add_labels_to_image(overlay_image, true, pred)\n",
    "\n",
    "        fig = plot_grad_cam(image_np, true, pred, overlay_image_with_labels)\n",
    "\n",
    "        # Log the image with labels to TensorBoard\n",
    "        overlay_image_tensor = transforms.ToTensor()(overlay_image_with_labels)\n",
    "        writer.add_image(f\"Grad-CAM/True_{true}_Pred_{pred}_{idx}\", overlay_image_tensor, global_step=idx)\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        plot_tensor = torch.from_numpy(\n",
    "            np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        ).reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plot_tensor = plot_tensor.permute(2, 0, 1) / 255.0  # Convert to CxHxW and normalize\n",
    "\n",
    "        # Log the plot to TensorBoard\n",
    "        writer.add_image(f\"Grad-CAM/Testing\", plot_tensor)\n",
    "\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces_in_images(images):\n",
    "    face_cascade = cv2.CascadeClassifier(\"/home/oem/eliza/DL/project/ai-art-detection/analysis/haarcascade_frontalface_default.xml\")\n",
    "    face_detected_count = 0\n",
    "    total_images = len(images)\n",
    "\n",
    "    for image in images:\n",
    "        # Convert tensor to numpy and denormalize\n",
    "        image_np = image.numpy().transpose(1, 2, 0)  # Convert (C, H, W) to (H, W, C)\n",
    "        image_np = (image_np * 255).astype(np.uint8)  # Convert to uint8 format\n",
    "        \n",
    "        # Convert to grayscale for Haar Cascade\n",
    "        gray_image = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "        if len(faces) > 0:\n",
    "            face_detected_count += 1\n",
    "\n",
    "    return face_detected_count, total_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "lVj5Py0WKTkf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# testing function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval() # evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    pred_list = []\n",
    "    true_list = []\n",
    "    misclassified_images = []\n",
    "    misclassified_preds = []\n",
    "    misclassified_true = []\n",
    "\n",
    "\n",
    "    # progress bar\n",
    "    pbar = tqdm(total=len(test_loader))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item() # sommo il loss di ogni batch\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True) # ottengo la predizione del modello\n",
    "            pred_list.extend(pred.cpu().numpy()) # aggiungo la predizione alla lista\n",
    "            true_list.extend(target.cpu().numpy()) # aggiungo il target alla lista\n",
    "            \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # aggiorno il contatore di classificazioni corrette\n",
    "\n",
    "            misclassified = ~pred.eq(target.view_as(pred)).squeeze()\n",
    "            if misclassified.any():\n",
    "                misclassified_images.extend(data[misclassified].cpu())\n",
    "                misclassified_preds.extend(pred[misclassified].cpu().numpy())\n",
    "                misclassified_true.extend(target[misclassified].cpu().numpy())\n",
    "\n",
    "            # update progress bar\n",
    "            pbar.update(1)\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    recall = recall_score(true_list, pred_list, average='macro')\n",
    "    precision = precision_score(true_list, pred_list, average='macro') \n",
    "    f1 = f1_score(true_list, pred_list, average='macro') \n",
    "    auc = roc_auc_score(true_list, pred_list) \n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%), Recall: {:.2f}%, Precision: {:.2f}%, F1: {:.2f}%, AUC: {:.2f}%\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), accuracy, recall*100, precision*100, f1*100, auc*100))\n",
    "    \n",
    "    # # Detect faces in misclassified images\n",
    "    # face_detected_count, total_images = detect_faces_in_images(misclassified_images)\n",
    "    # face_percentage = (face_detected_count / total_images) * 100 if total_images > 0 else 0\n",
    "    # print(f\"Faces detected in {face_detected_count}/{total_images} misclassified images ({face_percentage:.2f}%).\")\n",
    "\n",
    "    # log_misclassified_images(misclassified_images, misclassified_preds, misclassified_true, None, writer)\n",
    "    log_confusion_matrix(true_list, pred_list, 1, \"Testing\", writer)\n",
    "    # apply_grad_cam(model, misclassified_images, misclassified_preds, misclassified_true, writer)\n",
    "\n",
    "    writer.flush()\n",
    "    return accuracy, recall, precision, f1, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use this for logs:\n",
    "tensorboard --logdir=./runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FW2GesMHKf11",
    "outputId": "30e6c98e-eab6-4fe8-a641-9c61b3ac059e"
   },
   "outputs": [],
   "source": [
    "# accuracy,recall,precision,f1,auc = test_model(model,test_loader)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_cam(image, true, pred, overlay_image):\n",
    "    \"\"\"Plots the original image and Grad-CAM overlay image side by side.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plot the original image\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title(f\"Original Image\\nTrue: {true}, Pred: {pred}\")\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    # Plot the Grad-CAM overlay image\n",
    "    ax[1].imshow(overlay_image)\n",
    "    ax[1].set_title(f\"Grad-CAM Overlay last layer\\nTrue: {true}, Pred: {pred}\")\n",
    "    ax[1].axis('off')\n",
    "    return fig\n",
    "\n",
    "def apply_grad_cam_on_image(model, image_path, target_layer, device, true_label=None):\n",
    "    \"\"\"\n",
    "    Applies Grad-CAM++ on a specific image provided by its file path.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model.\n",
    "        image_path: Path to the image file.\n",
    "        target_layer: The target layer for Grad-CAM (e.g., a convolutional layer in the model).\n",
    "        device: Device to perform computations (CPU/GPU).\n",
    "        true_label: (Optional) True label of the image for reference.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Adjust size based on the model's input\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Adjust mean/std as per training\n",
    "    ])\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1)  # Get probabilities\n",
    "        predicted_class_idx = probabilities.argmax(dim=1).item()  # Get the predicted class index\n",
    "        predicted_prob = probabilities[0, predicted_class_idx].item() * 100  # Probability of the predicted class\n",
    "\n",
    "\n",
    "    cam = GradCAMPlusPlus(model=model, target_layers=[target_layer])\n",
    "    grayscale_cam = cam(input_tensor=input_tensor)[0]\n",
    "\n",
    "    # Convert the image to numpy for visualization\n",
    "    denormalized_image = denormalize_image_gradcam(\n",
    "        input_tensor.squeeze(0), mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]\n",
    "    ).cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    \n",
    "    # Overlay Grad-CAM on the original image\n",
    "    overlay_image = overlay_grad_cam_on_image(denormalized_image, grayscale_cam, alpha=0.5)\n",
    "\n",
    "    overlay_image_with_labels = add_labels_to_image(overlay_image, true_label, predicted_class_idx)\n",
    "\n",
    "    # Plot the original and Grad-CAM overlay images\n",
    "    plot_grad_cam(\n",
    "        denormalized_image,\n",
    "        true_label,\n",
    "        f\"Predicted: {predicted_class_idx} ({predicted_prob:.2f}%)\",\n",
    "        overlay_image_with_labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for ** or pow(): 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/oem/eliza/DL/project/data/deepfakeart/generated/adversarial/FGSM/similar-adversarial-3-adv_FGSM.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m target_layer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mconv_dw  \u001b[38;5;66;03m# Replace with your model's Grad-CAM target layer\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mapply_grad_cam_on_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[159], line 48\u001b[0m, in \u001b[0;36mapply_grad_cam_on_image\u001b[0;34m(model, image_path, target_layer, device, true_label)\u001b[0m\n\u001b[1;32m     44\u001b[0m     predicted_prob \u001b[38;5;241m=\u001b[39m probabilities[\u001b[38;5;241m0\u001b[39m, predicted_class_idx]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Probability of the predicted class\u001b[39;00m\n\u001b[1;32m     47\u001b[0m cam \u001b[38;5;241m=\u001b[39m GradCAMPlusPlus(model\u001b[38;5;241m=\u001b[39mmodel, target_layers\u001b[38;5;241m=\u001b[39m[target_layer])\n\u001b[0;32m---> 48\u001b[0m grayscale_cam \u001b[38;5;241m=\u001b[39m \u001b[43mcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Convert the image to numpy for visualization\u001b[39;00m\n\u001b[1;32m     51\u001b[0m denormalized_image \u001b[38;5;241m=\u001b[39m denormalize_image_gradcam(\n\u001b[1;32m     52\u001b[0m     input_tensor\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m]\n\u001b[1;32m     53\u001b[0m )\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:186\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_augmentation_smoothing(input_tensor, targets, eigen_smooth)\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:110\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     99\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# In most of the saliency attribution papers, the saliency is\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# computed with a single target layer.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Commonly it is the last convolutional layer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# use all conv layers for example, all Batchnorm layers,\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# or something else.\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m cam_per_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_cam_per_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_multi_layers(cam_per_layer)\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:141\u001b[0m, in \u001b[0;36mBaseCAM.compute_cam_per_layer\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(grads_list):\n\u001b[1;32m    139\u001b[0m     layer_grads \u001b[38;5;241m=\u001b[39m grads_list[i]\n\u001b[0;32m--> 141\u001b[0m cam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cam_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m cam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(cam, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    143\u001b[0m scaled \u001b[38;5;241m=\u001b[39m scale_cam_image(cam, target_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:66\u001b[0m, in \u001b[0;36mBaseCAM.get_cam_image\u001b[0;34m(self, input_tensor, target_layer, targets, activations, grads, eigen_smooth)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cam_image\u001b[39m(\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     59\u001b[0m     input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     eigen_smooth: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     65\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m---> 66\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cam_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# 2D conv\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(activations\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/pytorch_grad_cam/grad_cam_plusplus.py:19\u001b[0m, in \u001b[0;36mGradCAMPlusPlus.get_cam_weights\u001b[0;34m(self, input_tensor, target_layers, target_category, activations, grads)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cam_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     14\u001b[0m                     input_tensor,\n\u001b[1;32m     15\u001b[0m                     target_layers,\n\u001b[1;32m     16\u001b[0m                     target_category,\n\u001b[1;32m     17\u001b[0m                     activations,\n\u001b[1;32m     18\u001b[0m                     grads):\n\u001b[0;32m---> 19\u001b[0m     grads_power_2 \u001b[38;5;241m=\u001b[39m \u001b[43mgrads\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     20\u001b[0m     grads_power_3 \u001b[38;5;241m=\u001b[39m grads_power_2 \u001b[38;5;241m*\u001b[39m grads\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Equation 19 in https://arxiv.org/abs/1710.11063\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for ** or pow(): 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "image_path = \"/home/oem/eliza/DL/project/data/deepfakeart/generated/adversarial/FGSM/similar-adversarial-3-adv_FGSM.png\"\n",
    "target_layer = model.stages[-1].blocks[-1].conv_dw  # Replace with your model's Grad-CAM target layer\n",
    "apply_grad_cam_on_image(model, image_path, target_layer, device=device, true_label=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
