{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9INzPcbAUYOK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 16:48:44.628737: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-20 16:48:44.645449: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from PIL import Image,ImageFile\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import copy\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam import GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from captum.attr import IntegratedGradients\n",
    "import timm\n",
    "from torchvision.utils import make_grid\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "from torchcam.utils import overlay_mask\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x76f6d833ee30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_val = 420\n",
    "torch.manual_seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lgtLNNoslyOn"
   },
   "outputs": [],
   "source": [
    "base_data_dir = '../data/artifact'\n",
    "fake_dir = os.path.join(base_data_dir, 'generated')  # Fake artworks directory\n",
    "real_dir = os.path.join(base_data_dir, 'real')  # Real artworks directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eqZfdnU2h0c"
   },
   "source": [
    "ARTWORK DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JGgx0Qz7Abww"
   },
   "outputs": [],
   "source": [
    "#Class to manage artworks with respect to their authenticity\n",
    "\n",
    "class ArtworkDataset(Dataset):\n",
    "  def __init__(self,links,transform):\n",
    "      self.data = links\n",
    "      self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.data.index.shape[0]\n",
    "    \n",
    "  def __getitem__(self,idx):\n",
    "        img = Image.open(self.data.iloc[idx,0])\n",
    "        label_index = self.data.iloc[idx, 1]\n",
    "        if (img.mode != 'RGB'):\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "          img = self.transform(img)\n",
    "        return img, label_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YtBEgWxnk6cl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved at ../data/artifact/image_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a CSV file with paths to artwork images and their labels (real or fake)\n",
    "data = [] \n",
    "\n",
    "# Iterate over the fake artworks and add their paths and labels to the list\n",
    "for dirpath, dirnames, filenames in os.walk(fake_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".jpg\"): # only consider jpg files\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            data.append((filepath, \"0\"))\n",
    "\n",
    "\n",
    "# Iterate over the real artworks and add their paths and labels to the list\n",
    "for dirpath, dirnames, filenames in os.walk(real_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            data.append((filepath, \"1\"))  # Label 1 for real artworks\n",
    "\n",
    "# Convert the list \"data\" to a pandas dataframe\n",
    "df = pd.DataFrame(data, columns=[\"path\", \"label\"])\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "csv_output_path = os.path.join(base_data_dir, \"image_labels.csv\")\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "print(f\"CSV file saved at {csv_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5tSRTVLyp14"
   },
   "source": [
    "LOAD PRETRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMq_NPE1pbFG",
    "outputId": "a86e8690-f0ed-40df-f285-137995256b0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXt(\n",
      "  (stem): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (1): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (stages): Sequential(\n",
      "    (0): ConvNeXtStage(\n",
      "      (downsample): Identity()\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): ConvNeXtStage(\n",
      "      (downsample): Sequential(\n",
      "        (0): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ConvNeXtStage(\n",
      "      (downsample): Sequential(\n",
      "        (0): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (3): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (4): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (5): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (6): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (7): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (8): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (9): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (10): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (11): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (12): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (13): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (14): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (15): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (16): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (17): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (18): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (19): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (20): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (21): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (22): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (23): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (24): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (25): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (26): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ConvNeXtStage(\n",
      "      (downsample): Sequential(\n",
      "        (0): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm_pre): Identity()\n",
      "  (head): Sequential(\n",
      "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n",
      "    (norm): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('convnext_base',pretrained=True, num_classes=2)\n",
    "\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VCk48tcMq95u"
   },
   "outputs": [],
   "source": [
    "#Setting the model weights to non-trainable\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ekf7GcoTrIus"
   },
   "outputs": [],
   "source": [
    "#Make the last layer of the model trainable\n",
    "for p in model.head.parameters(): #instead of fc, we use head\n",
    "    p.requires_grad=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjxaiYnI0er8"
   },
   "source": [
    "SPLIT IN TRAINING AND VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "jAtnYc-86UaP",
    "outputId": "c1d498e5-effb-4815-a275-b3c4df241fc0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/artifact/generated/stylegan3-t-metface...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/artifact/generated/stylegan3-t-metface...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/artifact/generated/stylegan3-t-metface...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/artifact/generated/stylegan3-t-metface...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/artifact/generated/stylegan3-t-metface...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88146</th>\n",
       "      <td>../data/artifact/real/paul-albert-besnard_robe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88147</th>\n",
       "      <td>../data/artifact/real/nikolai-ge_christ-and-th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88148</th>\n",
       "      <td>../data/artifact/real/paul-bril_view-of-a-port...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88149</th>\n",
       "      <td>../data/artifact/real/felix-vallotton_the-port...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88150</th>\n",
       "      <td>../data/artifact/real/nicholas-roerich_tidings...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88151 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  label\n",
       "0      ../data/artifact/generated/stylegan3-t-metface...      0\n",
       "1      ../data/artifact/generated/stylegan3-t-metface...      0\n",
       "2      ../data/artifact/generated/stylegan3-t-metface...      0\n",
       "3      ../data/artifact/generated/stylegan3-t-metface...      0\n",
       "4      ../data/artifact/generated/stylegan3-t-metface...      0\n",
       "...                                                  ...    ...\n",
       "88146  ../data/artifact/real/paul-albert-besnard_robe...      1\n",
       "88147  ../data/artifact/real/nikolai-ge_christ-and-th...      1\n",
       "88148  ../data/artifact/real/paul-bril_view-of-a-port...      1\n",
       "88149  ../data/artifact/real/felix-vallotton_the-port...      1\n",
       "88150  ../data/artifact/real/nicholas-roerich_tidings...      1\n",
       "\n",
       "[88151 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df\n",
    "dataset['label'] = dataset['label'].astype(int)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "g60Xfpx800Oo"
   },
   "outputs": [],
   "source": [
    "#train, validation = train_test_split(dataset.values, stratify=dataset.values[:, 1], test_size=.3, random_state = 1) \n",
    "\n",
    "# Split the dataset into two parts (train + validation, and test)\n",
    "train_val_data, test = train_test_split(dataset.values, test_size=0.1, random_state=seed_val)\n",
    "\n",
    "# Split the train + validation part into training and validation sets\n",
    "train, validation = train_test_split(train_val_data, test_size=0.1, random_state=seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 7934\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test set size: {len(validation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uWkbE_RQ1pwq"
   },
   "outputs": [],
   "source": [
    "train_links = pd.DataFrame(train, columns = dataset.columns)\n",
    "validation_links = pd.DataFrame(validation, columns = dataset.columns)\n",
    "test_links = pd.DataFrame(test, columns = dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/artifact/real/felipe-de-vicente_art-67...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/artifact/generated/stylegan3-r-metface...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/artifact/real/johannes-itten_der-bachs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/artifact/generated/stylegan3-t-metface...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/artifact/real/stanley-spencer_kit-insp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path label\n",
       "0  ../data/artifact/real/felipe-de-vicente_art-67...     1\n",
       "1  ../data/artifact/generated/stylegan3-r-metface...     0\n",
       "2  ../data/artifact/real/johannes-itten_der-bachs...     1\n",
       "3  ../data/artifact/generated/stylegan3-t-metface...     0\n",
       "4  ../data/artifact/real/stanley-spencer_kit-insp...     1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_links[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jHAZeSr3IBk"
   },
   "source": [
    "#### DATA LOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GMOMejkn65Dp"
   },
   "outputs": [],
   "source": [
    "# old\n",
    "data_transforms = transforms.Compose([\n",
    "                                transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# new\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),                    # Resize to allow cropping\n",
    "    transforms.RandomCrop(224),                # Crop to 224x224\n",
    "    transforms.RandomHorizontalFlip(p=0.5),    # Random horizontal flip -- half of the images are mirrored; prevent the model from learning exact position-based cues.\n",
    "    transforms.RandomRotation(10),             # Small rotation for variability -- helps the model generalize across minor rotations, as real-world artworks and digital images might not always be perfectly aligned.\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1), # Minimal brightness/contrast -- A brightness and contrast jitter of ±10% introduces slight lighting variability without drastically changing color tones. This adjustment accounts for small lighting and contrast differences that can naturally occur in photographed or scanned artworks, which could otherwise become confounding factors.\n",
    "    transforms.ToTensor(),                     # Convert to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Standard normalization\n",
    "])\n",
    "\n",
    "# Validation/test transformations (no augmentation)\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize(224),                    # Resize directly to 224x224\n",
    "    transforms.CenterCrop(224),                # Center crop to ensure consistency\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image\n",
    "])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_set = ArtworkDataset(train_links[:1000], train_transforms)\n",
    "\n",
    "validation_set = ArtworkDataset(validation_links[:1000], val_test_transforms)\n",
    "\n",
    "test_set = ArtworkDataset(test_links[:1000], val_test_transforms)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, \n",
    "                               drop_last=False,num_workers=6, pin_memory = True)\n",
    "\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False, \n",
    "                               drop_last=False,num_workers=6, pin_memory = True)\n",
    "\n",
    "test_loader = DataLoader(test_set,batch_size=batch_size, shuffle = False,\n",
    "                              drop_last=False,num_workers=6, pin_memory = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ghMo_wYLTuq"
   },
   "source": [
    "**Early Stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KQv22H68LXcR"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.wait = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "                \n",
    "    def __call__(self, current_loss):\n",
    "        if self.best_loss is None or (current_loss - self.best_loss) < -self.min_delta:\n",
    "            self.best_loss = current_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            print(f\"INFO: Early stopping counter {self.wait} of {self.patience}\")\n",
    "            if self.wait >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions for visualisation and logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for logging the misclassified images in tensorboard\n",
    "def denormalize_image(tensor, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)):\n",
    "    \"\"\"Denormalizes a tensor image by applying the inverse of the normalization transform.\"\"\"\n",
    "    # Reshape mean and std to match the (C, H, W) shape of the tensor\n",
    "    if tensor.dim() == 4:\n",
    "        tensor = tensor[0]\n",
    "    mean = torch.tensor(mean).view(3, 1, 1).to(tensor.device)  # Match device as well\n",
    "    std = torch.tensor(std).view(3, 1, 1).to(tensor.device)\n",
    "    denormalized = tensor * std + mean\n",
    "    denormalized = denormalized.clamp(0, 1) # convert to [H, W, C] for matplotlib visualisation\n",
    "\n",
    "    return denormalized.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "def add_labels_to_image(image, true_label, pred_label):\n",
    "    \"\"\"Add true and predicted labels to the image.\"\"\"\n",
    "    # Convert the image to uint8 if it's in float format\n",
    "    if image.dtype != np.uint8:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "\n",
    "    # Convert from RGB to BGR for OpenCV\n",
    "    bgr_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Prepare text without brackets\n",
    "    text = f'True: {int(true_label)}, Pred: {int(pred_label)}'\n",
    "    cv2.putText(bgr_image, text, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Convert back to RGB before returning\n",
    "    return cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_misclassified_images(misclassified_images, misclassified_preds, misclassified_true, epoch, writer):\n",
    "    print(\"Logging misclassified images...\")\n",
    "    print(len(misclassified_images))\n",
    "    labeled_images = []\n",
    "\n",
    "    for i, img in enumerate(misclassified_images):\n",
    "        img_np = denormalize_image(img)  # Denormalize the image      \n",
    "\n",
    "        # Ensure the image is in the right format for TensorBoard\n",
    "        if img_np.shape[-1] == 1:  # Check if the image is grayscale\n",
    "            img_np = img_np.squeeze(axis=2)  # Remove the channel dimension if it is 1\n",
    "        elif img_np.shape[0] == 1:  # If the shape is (1, H, W)\n",
    "            img_np = img_np.squeeze(axis=0)  # Remove the batch dimension\n",
    "        img_np = np.clip(img_np, 0, 1)  # Ensure the values are between 0 and 1\n",
    "        img_np = (img_np * 255).astype(np.uint8)     # Scale to 0-255\n",
    "\n",
    "        true_label = misclassified_true[i]\n",
    "        pred_label = misclassified_preds[i]\n",
    "\n",
    "        labeled_image = add_labels_to_image(img_np, true_label, pred_label)\n",
    "        # labeled_image = labeled_image.transpose(2, 0, 1)\n",
    "        labeled_images.append(labeled_image)\n",
    "\n",
    "    # Log image to TensorBoard\n",
    "    writer.add_images(\n",
    "        f'Misclassified_Epoch_{epoch}',\n",
    "        np.array(labeled_images),\n",
    "        global_step=epoch,\n",
    "        dataformats='NHWC'\n",
    "    )\n",
    "\n",
    "\n",
    "def log_confusion_matrix(all_labels, all_preds, epoch, log_name, writer):\n",
    "    \"\"\"Log confusion matrix to TensorBoard.\"\"\"\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Plot confusion matrix using Seaborn\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Fake', 'True'], yticklabels=['Fake', 'True'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix - Epoch {epoch}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save to buffer and log\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    image = np.array(Image.open(buf))\n",
    "    writer.add_image(f'Confusion_Matrix', image, global_step=epoch, dataformats='HWC')\n",
    "    buf.close()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GRADCAM and Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXtBlock(\n",
      "  (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELU()\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.stages[-1].blocks[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_integrated_gradients_old(model, inputs, labels, device, epoch, writer):\n",
    "    \"\"\"\n",
    "    Applies Integrated Gradients to a single batch of inputs and logs a side-by-side plot (input + IG) to TensorBoard.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ig = IntegratedGradients(model)\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Perform IG for a single input (e.g., the first image in the batch)\n",
    "    input_example = inputs[0].unsqueeze(0)  # Add batch dimension\n",
    "    target_label = labels[0].item()  # Ground truth class\n",
    "\n",
    "    # Compute attributions\n",
    "    attributions = ig.attribute(input_example, target=target_label, n_steps=50)\n",
    "\n",
    "    # Convert tensors to numpy for visualization\n",
    "    input_example = input_example.squeeze().cpu().detach().numpy()\n",
    "    attributions = attributions.squeeze().cpu().detach().numpy()\n",
    "\n",
    "    # Normalize attributions for visualization\n",
    "    attributions = (attributions - attributions.min()) / (attributions.max() - attributions.min())\n",
    "\n",
    "    # Reshape input and attribution for RGB or Grayscale\n",
    "    input_example = input_example.transpose(1, 2, 0)  # CHW to HWC for RGB\n",
    "    if input_example.shape[2] != 3:  # Handle grayscale\n",
    "        input_example = np.repeat(input_example[:, :, np.newaxis], 3, axis=2)\n",
    "    attribution_overlay = attributions.transpose(1, 2, 0)\n",
    "\n",
    "    # Create a side-by-side plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(input_example)\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].imshow(attribution_overlay, cmap=\"viridis\")\n",
    "    axes[1].set_title(\"Integrated Gradients\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # Save the plot to a buffer\n",
    "    fig.canvas.draw()\n",
    "    buf = fig.canvas.tostring_rgb()\n",
    "    width, height = fig.canvas.get_width_height()\n",
    "    image = np.frombuffer(buf, dtype=np.uint8).reshape(height, width, 3).transpose(2, 0, 1)  # HWC to CHW\n",
    "\n",
    "    plt.close(fig)  # Close the plot to free memory\n",
    "\n",
    "    # Log the combined image to TensorBoard\n",
    "    writer.add_image(f\"IG/Input_and_IG\", image, epoch)\n",
    "\n",
    "    print(f\"Integrated Gradients side-by-side plot logged for epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_grad_cam(model, images, preds, true_labels, writer, log_name=\"Validation\"):\n",
    "    # Assume 'model' is in eval mode and the images are pre-processed\n",
    "    target_layer = model.stages[-1].blocks[-1].conv_dw\n",
    "    gradcam = GradCAMPlusPlus(model, target_layers=[target_layer])\n",
    "    for i, image in enumerate(images):\n",
    "        image_tensor = transforms.ToTensor()(image).unsqueeze(0).cuda()\n",
    "        true_label = true_labels[i]\n",
    "        \n",
    "        # Get Grad-CAM++ heatmap\n",
    "        cam = gradcam(image_tensor)\n",
    "        cam = cam.squeeze().cpu().detach().numpy()  # Remove batch dimension\n",
    "        \n",
    "        # Normalize the CAM output\n",
    "        cam = np.maximum(cam, 0)  # Set negative values to 0\n",
    "        cam = cam / cam.max()  # Normalize to [0, 1]\n",
    "\n",
    "        denormalized_image = denormalize_image(image_tensor.unsqueeze(0))\n",
    "        \n",
    "        # Overlay heatmap on the original image\n",
    "        cam = np.uint8(255 * cam)  # Convert to 0-255 range\n",
    "        cam = np.expand_dims(cam, axis=-1)  # Make it a 3-channel image\n",
    "        heatmap = np.repeat(cam, 3, axis=-1)\n",
    "        overlayed_image = np.uint8(denormalized_image * 0.5 + heatmap * 0.5)  # Blend the original image with heatmap\n",
    "\n",
    "        # Convert image to TensorBoard format\n",
    "        figure = plt.figure(figsize=(10, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(denormalized_image)\n",
    "        plt.title(f\"Original - True Label: {true_label}, Pred: {preds[i]}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(overlayed_image)\n",
    "        plt.title(f\"Grad-CAM++ Overlay\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        writer.add_figure(f'{log_name}/Grad-CAM++_{i}', figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gradcam(model, image_tensor, true_label, class_idx, writer, log_name=\"Validation\"):\n",
    "    \"\"\"\n",
    "    Apply Grad-CAM++ to visualize class-specific saliency maps and overlay them on the input image.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model.\n",
    "        target_layer: The name of the layer to use for Grad-CAM++.\n",
    "        image_tensor: The input image tensor (normalized, shape CxHxW).\n",
    "        true_label: The true label of the image (int).\n",
    "        class_idx: The predicted class index to visualize (int).\n",
    "        writer: TensorBoard writer for logging.\n",
    "        step: The current step (e.g., batch index or global step).\n",
    "        denormalize: A function to denormalize the image tensor for visualization.\n",
    "    \"\"\"\n",
    "    # Move model to eval mode\n",
    "    model.eval()\n",
    "    image_tensor.requires_grad = True\n",
    "    target_layer = model.stages[-1].blocks[-1].conv_dw\n",
    "\n",
    "    # Create Grad-CAM++ object\n",
    "    cam_extractor = SmoothGradCAMpp(model, target_layer)\n",
    "\n",
    "    # Detach the image for visualization\n",
    "    img = image_tensor.detach()\n",
    "\n",
    "    model(image_tensor)\n",
    "    \n",
    "    # Denormalize for visual output\n",
    "    img_np = denormalize_image(img)\n",
    "    img_np = (img_np * 255).astype('uint8')\n",
    "    pil_image = Image.fromarray(img_np)\n",
    "\n",
    "    if not isinstance(class_idx, (list, np.ndarray)):\n",
    "        class_idx = [class_idx]\n",
    "\n",
    "    # Generate Grad-CAM++ activation maps\n",
    "    cams = cam_extractor(class_idx, img.unsqueeze(0))\n",
    "    cam_map = cams[0].squeeze(0).cpu()\n",
    "\n",
    "    # Overlay Grad-CAM++ heatmap on the original image\n",
    "    heatmap = overlay_mask(pil_image, Image.fromarray((cam_map.numpy() * 255).astype('uint8')), alpha=0.5)\n",
    "\n",
    "    # Plot original image and overlayed heatmap\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].imshow(pil_image)\n",
    "    ax[0].set_title(f\"Original Image (Label: {true_label})\")\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[1].imshow(heatmap)\n",
    "    ax[1].set_title(f\"Grad-CAM++ (Class: {class_idx})\")\n",
    "    ax[1].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Log images to TensorBoard\n",
    "    writer.add_figure(f\"GradCAM++/{log_name}\", fig)\n",
    "\n",
    "    plt.close(fig)  # Close the plot to free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_integrated_gradients(\n",
    "    model, input_tensor, target_label, baseline=None, writer=None, log_name = \"Validation\"):\n",
    "    \"\"\"\n",
    "    Visualizes Integrated Gradients (IG) for a single input image.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model.\n",
    "        input_tensor: Input image tensor (C, H, W) normalized for the model.\n",
    "        target_label: Target label for IG attribution.\n",
    "        baseline: Baseline for IG. Defaults to zero baseline.\n",
    "        writer: TensorBoard SummaryWriter object.\n",
    "        log_name: Name for logging the figure (default: \"Validation\").\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the target_label is a PyTorch tensor of type long\n",
    "    target_label = torch.tensor(target_label, dtype=torch.long).to(input_tensor.device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Ensure that the input tensor requires gradients\n",
    "    if input_tensor.dim() == 3:  # Shape: [C, H, W]\n",
    "        input_tensor = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "    input_tensor.requires_grad_()  # Enable gradients for backpropagation\n",
    "\n",
    "\n",
    "    input_tensor.requires_grad_()  # Ensure the input tensor requires gradients\n",
    "\n",
    "    # Prepare baseline (default is zero image)\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(input_tensor).to(input_tensor.device)\n",
    "    baseline.requires_grad = False\n",
    "\n",
    "    \n",
    "    # Compute attributions using Integrated Gradients\n",
    "    ig = IntegratedGradients(model)\n",
    "    attributions, _ = ig.attribute(\n",
    "        inputs=input_tensor, \n",
    "        baselines=baseline, \n",
    "        target=target_label, \n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "\n",
    "    print(f\"Attributions shape before processing: {attributions.shape}\")\n",
    "\n",
    "    \n",
    "    # Process the attributions\n",
    "    attributions = attributions.squeeze(0).cpu().detach()  # Shape: (C, H, W)\n",
    "    print(f\"Attributions shape after squeeze: {attributions.shape}\")\n",
    "\n",
    "    \n",
    "    # Normalize IG attributions to [0, 1] for visualization\n",
    "    attributions_normalized = (attributions - attributions.min()) / (attributions.max() - attributions.min())\n",
    "    attributions_normalized = attributions_normalized.permute(1, 2, 0).numpy()  # Shape: (H, W, C)\n",
    "    \n",
    "    # Denormalize the input image for visualization\n",
    "    input_image_denormalized = denormalize_image(input_tensor.squeeze(0)).permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # Plotting the results\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].imshow(input_image_denormalized)\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('Denormalized Input Image')\n",
    "\n",
    "    ax[1].imshow(input_image_denormalized, alpha=0.6)  # Overlay original image\n",
    "    ax[1].imshow(attributions_normalized, cmap='hot', alpha=0.4)  # Overlay IG heatmap\n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title('Integrated Gradients Attributions')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure to TensorBoard if writer is provided\n",
    "    plt.show()\n",
    "    writer.add_figure(f\"Integrated Gradients/{log_name}\", fig)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING AND VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device, epoch, log_name, writer):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Iterate over training data\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch} Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        outputs = nn.Softmax(dim=1)(outputs)  # Apply softmax for classification\n",
    "        preds = torch.argmax(outputs, dim = 1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # Compute loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / (len(train_loader) * train_loader.batch_size)\n",
    "    epoch_acc = running_corrects.double() / (len(train_loader) * train_loader.batch_size)\n",
    "\n",
    "    # Log statistics\n",
    "    writer.add_scalar(f'Loss/Train/{log_name}', epoch_loss, epoch)\n",
    "    writer.add_scalar(f'Accuracy/Train/{log_name}', epoch_acc, epoch)\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "    return epoch_acc, epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, validation_loader, criterion, device, epoch, num_epochs, log_name, writer):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    misclassified_images = []\n",
    "    misclassified_preds = []\n",
    "    misclassified_true = []\n",
    "\n",
    "    # Disable gradient computation for validation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(validation_loader, desc=f\"Epoch {epoch} Validation\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = nn.Softmax(dim=1)(outputs)  # Apply softmax for classification\n",
    "            #_, preds = torch.max(outputs, 1)\n",
    "            preds = torch.argmax(outputs, dim = 1)\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Collect all predictions and labels for confusion matrix\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "         \n",
    "            # Collect misclassified images\n",
    "            misclassified = preds != labels.data\n",
    "            if misclassified.any():\n",
    "                misclassified_images.extend(inputs[misclassified].cpu())\n",
    "                misclassified_preds.extend(preds[misclassified].cpu().numpy())\n",
    "                misclassified_true.extend(labels[misclassified].cpu().numpy())\n",
    "\n",
    "    # Compute loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / (len(validation_loader) * validation_loader.batch_size)\n",
    "    epoch_acc = running_corrects.double() / (len(validation_loader) * validation_loader.batch_size)\n",
    "\n",
    "    # Log statistics\n",
    "    writer.add_scalar(f'Loss/Validation/{log_name}', epoch_loss, epoch)\n",
    "    writer.add_scalar(f'Accuracy/Validation/{log_name}', epoch_acc, epoch)\n",
    "\n",
    "    print(f\"Validation Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    log_confusion_matrix(all_labels, all_preds, epoch, log_name, writer)\n",
    "    \n",
    "    # Log misclassified images (with confusion matrix info)\n",
    "    # log_misclassified_images(misclassified_images, misclassified_preds, misclassified_true, epoch, writer)\n",
    "\n",
    "    # Apply Grad-CAM and IG to a subset of misclassified images\n",
    "    subset_size = min(50, len(misclassified_images))\n",
    "    if subset_size > 0:\n",
    "        subset_indices = np.random.choice(len(misclassified_images), size=subset_size, replace=False)\n",
    "        subset_images = torch.stack([misclassified_images[i] for i in subset_indices])\n",
    "        subset_preds = [misclassified_preds[i] for i in subset_indices]\n",
    "        subset_true = [misclassified_true[i] for i in subset_indices]\n",
    "\n",
    "        for i, (image, pred, true) in enumerate(zip(subset_images, subset_preds, subset_true)):\n",
    "            log_gradcam(model, image.unsqueeze(0).to(device), pred, true, writer, \"Validation\")\n",
    "            # apply_grad_cam(model, image.unsqueeze(0).to(device), pred, true, writer, \"Validation\")\n",
    "\n",
    "            log_integrated_gradients(model, image.to(device), true, None, writer, \"Validation\")\n",
    "\n",
    "\n",
    "\n",
    "    return epoch_acc, epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(model, train_loader, validation_loader, criterion, optimizer, scheduler, early_stop, num_epochs=100, \n",
    "              log_name='run1', device='cuda', writer=None):\n",
    "    best_model = copy.deepcopy(model)\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    stop = False\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        if stop:\n",
    "            break\n",
    "        print(f'Epoch {epoch}/{num_epochs}')\n",
    "        print('-'*120)\n",
    "\n",
    "        # Training phase\n",
    "        train_acc, train_loss = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device, epoch, log_name, writer)\n",
    "\n",
    "        # Validation phase\n",
    "        val_acc, val_loss = validate_one_epoch(model, validation_loader, criterion, device, epoch, num_epochs, log_name, writer)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        early_stop(val_loss)\n",
    "        stop = early_stop.early_stop\n",
    "\n",
    "        print('-'*120)\n",
    "\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    print(f'Best epoch: {best_epoch:03d}')\n",
    "    \n",
    "    # Save the best model state when training stops\n",
    "    model_path = os.path.join('saved_models/convnext_model', 'RealArt_vs_FakeArt_convnext_base.pt')\n",
    "    # torch.save(best_model.state_dict(), model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWDxhbLaMH-G",
    "outputId": "a7d86773-ec64-47d3-d598-0d1de9fafeac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a new model...\n",
      "Epoch 1/30\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 32/32 [00:03<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5469 Acc: 0.8584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation: 100%|██████████| 32/32 [00:02<00:00, 11.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4760 Acc: 0.8750\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 78.00 MiB. GPU 0 has a total capacity of 11.75 GiB of which 74.50 MiB is free. Including non-PyTorch memory, this process has 11.53 GiB memory in use. Of the allocated memory 11.06 GiB is allocated by PyTorch, and 154.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m    ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m    \u001b[38;5;66;03m# Train and save the best model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m    best_model_head \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     17\u001b[0m      \u001b[38;5;66;03m# Model already exists; load it\u001b[39;00m\n\u001b[1;32m     18\u001b[0m      \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading pre-trained model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 18\u001b[0m, in \u001b[0;36mfine_tune\u001b[0;34m(model, train_loader, validation_loader, criterion, optimizer, scheduler, early_stop, num_epochs, log_name, device, writer)\u001b[0m\n\u001b[1;32m     15\u001b[0m train_acc, train_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device, epoch, log_name, writer)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m val_acc, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Early stopping\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_acc \u001b[38;5;241m>\u001b[39m best_acc:\n",
      "Cell \u001b[0;32mIn[24], line 69\u001b[0m, in \u001b[0;36mvalidate_one_epoch\u001b[0;34m(model, validation_loader, criterion, device, epoch, num_epochs, log_name, writer)\u001b[0m\n\u001b[1;32m     66\u001b[0m         log_gradcam(model, image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), pred, true, writer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;66;03m# apply_grad_cam(model, image.unsqueeze(0).to(device), pred, true, writer, \"Validation\")\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m         \u001b[43mlog_integrated_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mValidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epoch_acc, epoch_loss\n",
      "Cell \u001b[0;32mIn[22], line 39\u001b[0m, in \u001b[0;36mlog_integrated_gradients\u001b[0;34m(model, input_tensor, target_label, baseline, writer, log_name)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Compute attributions using Integrated Gradients\u001b[39;00m\n\u001b[1;32m     38\u001b[0m ig \u001b[38;5;241m=\u001b[39m IntegratedGradients(model)\n\u001b[0;32m---> 39\u001b[0m attributions, _ \u001b[38;5;241m=\u001b[39m \u001b[43mig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttributions shape before processing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattributions\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Process the attributions\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/captum/log/__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/captum/attr/_core/integrated_gradients.py:286\u001b[0m, in \u001b[0;36mIntegratedGradients.attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    274\u001b[0m     attributions \u001b[38;5;241m=\u001b[39m _batch_attribution(\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m         num_examples,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     attributions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaselines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_convergence_delta:\n\u001b[1;32m    296\u001b[0m     start_point, end_point \u001b[38;5;241m=\u001b[39m baselines, inputs\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/captum/attr/_core/integrated_gradients.py:351\u001b[0m, in \u001b[0;36mIntegratedGradients._attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[1;32m    348\u001b[0m expanded_target \u001b[38;5;241m=\u001b[39m _expand_target(target, n_steps)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaled_features_tpl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_additional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[1;32m    360\u001b[0m scaled_grads \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    361\u001b[0m     grad\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(n_steps, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(step_sizes)\u001b[38;5;241m.\u001b[39mview(n_steps, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(grad\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads\n\u001b[1;32m    364\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/captum/_utils/gradient.py:112\u001b[0m, in \u001b[0;36mcompute_gradients\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03mComputes gradients of the output with respect to inputs for an\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03marbitrary forward function.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m                arguments) if no additional arguments are required\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# runs forward pass\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget not provided when necessary, cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m take gradient with respect to multiple outputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# contains batch_size * #steps elements\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/captum/_utils/common.py:531\u001b[0m, in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    528\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _format_inputs(inputs)\n\u001b[1;32m    529\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[0;32m--> 531\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _select_targets(output, target)\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1566\u001b[0m     ):\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/timm/models/convnext.py:411\u001b[0m, in \u001b[0;36mConvNeXt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 411\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/timm/models/convnext.py:398\u001b[0m, in \u001b[0;36mConvNeXt.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    397\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem(x)\n\u001b[0;32m--> 398\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_pre(x)\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/timm/models/convnext.py:250\u001b[0m, in \u001b[0;36mConvNeXtStage.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    248\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/timm/models/convnext.py:184\u001b[0m, in \u001b[0;36mConvNeXtBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    182\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    183\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m--> 184\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/timm/models/layers/mlp.py:27\u001b[0m, in \u001b[0;36mMlp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[1;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 78.00 MiB. GPU 0 has a total capacity of 11.75 GiB of which 74.50 MiB is free. Including non-PyTorch memory, this process has 11.53 GiB memory in use. Of the allocated memory 11.06 GiB is allocated by PyTorch, and 154.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join('saved_models/convnext_model', 'RealArt_vs_FakeArt_convnext_base1.pt')\n",
    "writer = SummaryWriter(\"runs/gradcam\")\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "   print(\"Training a new model...\")\n",
    "   criterion = nn.CrossEntropyLoss()\n",
    "   optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "   scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "   early_stop = EarlyStopping(patience = 3, min_delta = 0.001)\n",
    "   ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "   \n",
    "   # Train and save the best model\n",
    "   best_model_head = fine_tune(model, train_loader, validation_loader, criterion, optimizer, scheduler, \n",
    "                               early_stop, num_epochs = 30, writer = writer)\n",
    "else: \n",
    "     # Model already exists; load it\n",
    "     print(\"Loading pre-trained model...\")\n",
    "     model = timm.create_model('convnext_base',pretrained=True, num_classes=2) \n",
    "     model.load_state_dict(torch.load(model_path))\n",
    "     model.to(device)\n",
    "     print(\"Model loaded successfully from:\", model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ra1QN4PE6WRX"
   },
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVj5Py0WKTkf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 276/276 [00:40<00:00, 12.20it/s]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# testing function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval() # evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    pred_list = []\n",
    "    true_list = []\n",
    "    misclassified_images = []\n",
    "    misclassified_preds = []\n",
    "    misclassified_true = []\n",
    "\n",
    "\n",
    "    # progress bar\n",
    "    pbar = tqdm(total=len(test_loader))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item() # sommo il loss di ogni batch\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True) # ottengo la predizione del modello\n",
    "            pred_list.extend(pred.cpu().numpy()) # aggiungo la predizione alla lista\n",
    "            true_list.extend(target.cpu().numpy()) # aggiungo il target alla lista\n",
    "            \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # aggiorno il contatore di classificazioni corrette\n",
    "\n",
    "            misclassified = ~pred.eq(target.view_as(pred)).squeeze()\n",
    "            if misclassified.any():\n",
    "                misclassified_images.extend(data[misclassified].cpu())\n",
    "                misclassified_preds.extend(pred[misclassified].cpu().numpy())\n",
    "                misclassified_true.extend(target[misclassified].cpu().numpy())\n",
    "\n",
    "            # update progress bar\n",
    "            pbar.update(1)\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    recall = recall_score(true_list, pred_list, average='macro')\n",
    "    precision = precision_score(true_list, pred_list, average='macro') \n",
    "    f1 = f1_score(true_list, pred_list, average='macro') \n",
    "    auc = roc_auc_score(true_list, pred_list) \n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%), Recall: {:.2f}%, Precision: {:.2f}%, F1: {:.2f}%, AUC: {:.2f}%\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), accuracy, recall*100, precision*100, f1*100, auc*100))\n",
    "    \n",
    "    log_misclassified_images(misclassified_images, misclassified_preds, misclassified_true, None, writer)\n",
    "\n",
    "    if len(misclassified_images) > 0:\n",
    "            for i, (image, pred, true) in enumerate(zip(misclassified_images, misclassified_preds, misclassified_true)):\n",
    "                # Apply Grad-CAM\n",
    "                apply_grad_cam(model, image.unsqueeze(0).to(device), pred, true, writer, \"Testing\")\n",
    "                # Log Integrated Gradients\n",
    "                log_integrated_gradients(model, image.to(device), target_label=true, baseline = None,writer=writer, log_name=\"Testing\")\n",
    "\n",
    "    writer.flush()\n",
    "    return accuracy, recall, precision, f1, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use this for logs:\n",
    "tensorboard --logdir=./runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FW2GesMHKf11",
    "outputId": "30e6c98e-eab6-4fe8-a641-9c61b3ac059e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_333378/1298213628.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  text = f'True: {int(true_label)}, Pred: {int(pred_label)}'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0752, Accuracy: 8643/8816 (98.04%), Recall: 97.90%, Precision: 98.10%, F1: 98.00%, AUC: 97.90%\n",
      "\n",
      "Logging misclassified images...\n",
      "173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 276/276 [00:25<00:00, 10.63it/s]\n"
     ]
    }
   ],
   "source": [
    "accuracy,recall,precision,f1,auc = test_model(model,test_loader)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
